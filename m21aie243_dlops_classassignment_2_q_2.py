# -*- coding: utf-8 -*-
"""M21AIE243_DLOps_ClassAssignment_2_Q_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgK0-VguErilRi_BZTuiqPa4WByMuRAP
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, datasets, transforms

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model = models.resnet50(pretrained=True)

num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 10)
model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)

model = model.to(device)

optimizers = {
    "Adam": optim.Adam(model.parameters(), lr=0.001),
    "Adagrad": optim.Adagrad(model.parameters(), lr=0.001),
    "Adadelta": optim.Adadelta(model.parameters(), lr=1.0),
}

schedulers = {
    "Adam": optim.lr_scheduler.StepLR(optimizers["Adam"], step_size=5, gamma=0.1),
    "Adagrad": optim.lr_scheduler.StepLR(optimizers["Adagrad"], step_size=5, gamma=0.1),
    "Adadelta": optim.lr_scheduler.StepLR(optimizers["Adadelta"], step_size=5, gamma=0.1),
}

def train_model(optimizer, scheduler, trainloader, num_epochs=15, subsample_factor=5.0):
    train_loss_history = []
    train_acc_history = []

    num_samples = int(len(trainloader.dataset) * subsample_factor)
    subset_sampler = torch.utils.data.RandomSampler(trainloader.dataset, num_samples=num_samples, replacement=False)
    subset_batch_sampler = torch.utils.data.BatchSampler(subset_sampler, batch_size=trainloader.batch_size, drop_last=False)

    # Create a new DataLoader with the subset batch sampler
    subset_trainloader = torch.utils.data.DataLoader(trainloader.dataset, batch_sampler=subset_batch_sampler)

    for epoch in range(num_epochs):
        running_loss = 0.0
        correct = 0
        total = 0

        for i, data in enumerate(subset_trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)  # Move data to GPU
            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(subset_trainloader)
        epoch_acc = correct / total

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}")

        train_loss_history.append(epoch_loss)
        train_acc_history.append(epoch_acc)

        scheduler.step()

    return train_loss_history, train_acc_history

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = datasets.STL10(root='./data', split='train', download=True, transform=transform)

batch_size = 64
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)

criterion = nn.CrossEntropyLoss()

print("Training with optimizer and learning rate scheduling:")
model.train()
for optimizer_name, optimizer in optimizers.items():
    scheduler = schedulers[optimizer_name]
    print(f"Training with {optimizer_name} optimizer and learning rate scheduling:")
    train_loss_history, train_acc_history = train_model(optimizer, scheduler, trainloader)

testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)

def test_model(model, testloader):
    model.eval()
    correct_top1 = 0
    correct_top5 = 0
    total = 0

    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.topk(outputs, k=5, dim=1)
            total += labels.size(0)
            correct_top1 += (predicted[:, 0] == labels).sum().item()
            correct_top5 += (predicted == labels.view(-1, 1)).sum().item()

    top1_accuracy = correct_top1 / total
    top5_accuracy = correct_top5 / total

    print(f'Top-1 Test Accuracy: {top1_accuracy:.4f}')



    print(f'Top-5 Test Accuracy: {top5_accuracy:.4f}')

import matplotlib.pyplot as plt

# Plot training loss
plt.plot(train_loss_history, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.legend()
plt.show()

# Plot training accuracy
plt.plot(train_acc_history, label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Curve')
plt.legend()
plt.show()

